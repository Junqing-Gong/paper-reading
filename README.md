<h1 align="center">NLP-Paper-Reading</h1>
<div align="center">

[![Blog](https://img.shields.io/badge/CSDN-@JunqingGong-blue.svg?style=social)](https://blog.csdn.net/m0_53322388?spm=1010.2135.3001.5421)
[![Paper Support](https://img.shields.io/badge/paper-repo-blue.svg?style=social)](https://github.com/Junqing-Gong/hello-world)
![Stars Thanks](https://img.shields.io/badge/Stars-thanks-brightgreen.svg?style=social&logo=trustpilot)
![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=social&logo=appveyor)

</div>

刚开始涉足深度学习和nlp领域，将阅读笔记记录于此。

**☞ 论文按照阅读顺序排放**

# Contents | 内容
+ [综述](#summarize--综述)
+ [预训练](#pretraining--预训练)
+ [命名实体识别](#ner--命名实体识别)
+ [计算机视觉](#cv--计算机视觉)
+ [仓库](#repositories--仓库)


# Summarize | 综述
+ [A Survey on Deep Learning for Named Entity Recognition](https://ieeexplore.ieee.org/abstract/document/9039685)：实体识别综述 | Li et al,2020

# Pretraining | 预训练
+ [Attention Is All You Need](https://ieeexplore.ieee.org/abstract/document/9039685)：transformer | Vaswani et al,2017

+ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)：BERT | Devlin et al,2019

+ [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)：GPT | Radford et al,2018

# NER | 命名实体识别
+ [Discontinuous Named Entity Recognition as Maximal Clique Discovery](https://arxiv.org/abs/2106.00218)：极大团 | Wang et al,2021

# CV | 计算机视觉
+ [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)：AlexNet | Alex et al,2012

+ [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)：ResNet | Kaiming et al,2015

# Repositories | 仓库

